{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d5a078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Imports ----------\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import re\n",
    "import asyncio\n",
    "import json\n",
    "import signal\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Optional\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import dns.resolver\n",
    "import whois\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "import gzip\n",
    "import bz2\n",
    "import argparse\n",
    "\n",
    "\n",
    "def get_file_size_mb(file_path):\n",
    "    \"\"\"Get file size in MB\"\"\"\n",
    "    return os.path.getsize(file_path) / (1024 * 1024)\n",
    "\n",
    "def analyze_file(file_path):\n",
    "    \"\"\"Analyze processed file and show statistics\"\"\"\n",
    "    print(f\"\\nðŸ“Š File Analysis: {file_path}\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"âŒ File not found!\")\n",
    "        return\n",
    "    \n",
    "    size_mb = get_file_size_mb(file_path)\n",
    "    print(f\"ðŸ“ File size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    # Determine file type and load data\n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_path.endswith('.jsonl'):\n",
    "        records = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    records.append(json.loads(line))\n",
    "        df = pd.DataFrame(records)\n",
    "    else:\n",
    "        print(\"âŒ Unsupported file format\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ðŸ“ˆ Total domains: {len(df)}\")\n",
    "    \n",
    "    # Analyze data completeness\n",
    "    if 'Domain' in df.columns:\n",
    "        # Count domains with meaningful data\n",
    "        meaningful_domains = 0\n",
    "        for _, row in df.iterrows():\n",
    "            non_na_fields = sum(1 for v in row.values if str(v) not in ['NA', 'No', 'disabled', ''])\n",
    "            if non_na_fields >= 3:  # Domain + at least 2 other fields\n",
    "                meaningful_domains += 1\n",
    "        \n",
    "        print(f\"âœ… Domains with data: {meaningful_domains} ({meaningful_domains/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        # Show top data sources\n",
    "        if 'Posts API Status' in df.columns:\n",
    "            wp_success = len(df[df['Posts API Status'] == 'success'])\n",
    "            print(f\"ðŸ”Œ WordPress API success: {wp_success} ({wp_success/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        if 'Has Contact Form' in df.columns:\n",
    "            contact_forms = len(df[df['Has Contact Form'] == 'Yes'])\n",
    "            print(f\"ðŸ“ Contact forms found: {contact_forms} ({contact_forms/len(df)*100:.1f}%)\")\n",
    "\n",
    "def split_file_by_size(file_path, max_size_mb=50):\n",
    "    \"\"\"Split large file into smaller chunks\"\"\"\n",
    "    print(f\"\\nâœ‚ï¸ Splitting {file_path} into {max_size_mb}MB chunks...\")\n",
    "    \n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Calculate rows per chunk\n",
    "        total_size = get_file_size_mb(file_path)\n",
    "        rows_per_chunk = int(len(df) * (max_size_mb / total_size))\n",
    "        \n",
    "        chunks = [df[i:i+rows_per_chunk] for i in range(0, len(df), rows_per_chunk)]\n",
    "        \n",
    "        base_name = file_path.replace('.csv', '')\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_file = f\"{base_name}_part{i+1}.csv\"\n",
    "            chunk.to_csv(chunk_file, index=False)\n",
    "            print(f\"ðŸ“ Created: {chunk_file} ({len(chunk)} domains, {get_file_size_mb(chunk_file):.1f}MB)\")\n",
    "    \n",
    "    elif file_path.endswith('.jsonl'):\n",
    "        # For JSONL, split by line count\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        total_size = get_file_size_mb(file_path)\n",
    "        lines_per_chunk = int(len(lines) * (max_size_mb / total_size))\n",
    "        \n",
    "        base_name = file_path.replace('.jsonl', '')\n",
    "        chunk_num = 1\n",
    "        \n",
    "        for i in range(0, len(lines), lines_per_chunk):\n",
    "            chunk_lines = lines[i:i+lines_per_chunk]\n",
    "            chunk_file = f\"{base_name}_part{chunk_num}.jsonl\"\n",
    "            \n",
    "            with open(chunk_file, 'w') as f:\n",
    "                f.writelines(chunk_lines)\n",
    "            \n",
    "            print(f\"ðŸ“ Created: {chunk_file} ({len(chunk_lines)} domains, {get_file_size_mb(chunk_file):.1f}MB)\")\n",
    "            chunk_num += 1\n",
    "\n",
    "def compress_file(file_path, compression='gzip'):\n",
    "    \"\"\"Compress a file\"\"\"\n",
    "    print(f\"\\nðŸ—œï¸ Compressing {file_path} with {compression}...\")\n",
    "    \n",
    "    original_size = get_file_size_mb(file_path)\n",
    "    \n",
    "    if compression == 'gzip':\n",
    "        with open(file_path, 'rb') as f_in:\n",
    "            with gzip.open(f\"{file_path}.gz\", 'wb') as f_out:\n",
    "                f_out.writelines(f_in)\n",
    "        compressed_file = f\"{file_path}.gz\"\n",
    "    elif compression == 'bz2':\n",
    "        with open(file_path, 'rb') as f_in:\n",
    "            with bz2.open(f\"{file_path}.bz2\", 'wb') as f_out:\n",
    "                f_out.writelines(f_in)\n",
    "        compressed_file = f\"{file_path}.bz2\"\n",
    "    else:\n",
    "        print(\"âŒ Unsupported compression format\")\n",
    "        return\n",
    "    \n",
    "    compressed_size = get_file_size_mb(compressed_file)\n",
    "    compression_ratio = (1 - compressed_size/original_size) * 100\n",
    "    \n",
    "    print(f\"âœ… Compressed: {compressed_file}\")\n",
    "    print(f\"ðŸ“Š Size reduction: {original_size:.1f}MB â†’ {compressed_size:.1f}MB ({compression_ratio:.1f}% smaller)\")\n",
    "\n",
    "def clean_data(file_path, output_file=None):\n",
    "    \"\"\"Remove domains with minimal data to reduce file size\"\"\"\n",
    "    print(f\"\\nðŸ§¹ Cleaning data in {file_path}...\")\n",
    "    \n",
    "    if output_file is None:\n",
    "        base_name, ext = os.path.splitext(file_path)\n",
    "        output_file = f\"{base_name}_cleaned{ext}\"\n",
    "    \n",
    "    if file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_path.endswith('.jsonl'):\n",
    "        records = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    records.append(json.loads(line))\n",
    "        df = pd.DataFrame(records)\n",
    "    \n",
    "    original_count = len(df)\n",
    "    \n",
    "    # Keep only domains with meaningful data\n",
    "    cleaned_df = df.copy()\n",
    "    rows_to_keep = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        meaningful_fields = 0\n",
    "        for col, value in row.items():\n",
    "            if col != 'Domain' and str(value) not in ['NA', 'No', 'disabled', '']:\n",
    "                meaningful_fields += 1\n",
    "        \n",
    "        # Keep if has at least 3 meaningful fields (excluding domain)\n",
    "        if meaningful_fields >= 3:\n",
    "            rows_to_keep.append(idx)\n",
    "    \n",
    "    cleaned_df = df.loc[rows_to_keep]\n",
    "    \n",
    "    # Save cleaned data\n",
    "    if output_file.endswith('.csv'):\n",
    "        cleaned_df.to_csv(output_file, index=False)\n",
    "    elif output_file.endswith('.jsonl'):\n",
    "        with open(output_file, 'w') as f:\n",
    "            for record in cleaned_df.to_dict('records'):\n",
    "                f.write(json.dumps(record, default=str) + '\\n')\n",
    "    \n",
    "    original_size = get_file_size_mb(file_path)\n",
    "    cleaned_size = get_file_size_mb(output_file)\n",
    "    \n",
    "    print(f\"âœ… Cleaned file: {output_file}\")\n",
    "    print(f\"ðŸ“Š Domains: {original_count} â†’ {len(cleaned_df)} ({len(cleaned_df)/original_count*100:.1f}% kept)\")\n",
    "    print(f\"ðŸ“Š Size: {original_size:.1f}MB â†’ {cleaned_size:.1f}MB ({(1-cleaned_size/original_size)*100:.1f}% smaller)\")\n",
    "\n",
    "\n",
    "try:\n",
    "    import aiohttp\n",
    "except ImportError:\n",
    "    print(\"âŒ Missing dependency: aiohttp. Please install it: pip install aiohttp\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ---------- Configuration ----------\n",
    "def get_config(preset: str = \"balanced\"):\n",
    "    \"\"\"Return configuration settings with performance presets\"\"\"\n",
    "    base_config = {\n",
    "        'input_file': r\"sample.csv\",\n",
    "        'output_file': r\"processed.csv\",\n",
    "        'checkpoint_file': r\"checkpoint.json\",\n",
    "        'shutdown_file': r\"stop_processing.txt\",\n",
    "        'batch_save_interval': 5,  # Save progress every N domains\n",
    "        'MAX_PAGES_PER_DOMAIN': 250,\n",
    "        'MAX_CONCURRENCY': 20,\n",
    "        'REQUEST_TIMEOUT_SECONDS': 12,\n",
    "        'USER_AGENT': \"Mozilla/5.0 (compatible; LeadDiscoveryBot/1.0; +https://example.com/bot)\",\n",
    "        'RECENT_DAYS_THRESHOLD': 180,\n",
    "        'MAX_EMAILS_IN_OUTPUT': 10,\n",
    "        'MAX_SOCIALS_IN_OUTPUT': 15,\n",
    "        'COMMON_PAGES': [\"\", \"contact\", \"contact-us\", \"support\", \"help\", \"about\", \"about-us\", \"team\"],\n",
    "        'SOCIAL_DOMAINS': [\"facebook.com\", \"linkedin.com\", \"twitter.com\", \"x.com\", \"instagram.com\"],\n",
    "        \n",
    "        # Feature flags - enable/disable specific operations\n",
    "        'ENABLE_WHOIS': True,\n",
    "        'ENABLE_DNS_MX': True,\n",
    "        'ENABLE_CRAWLING': True,\n",
    "        'ENABLE_WORDPRESS_API': True,\n",
    "        'ENABLE_POSTS_API': True,\n",
    "        'ENABLE_PAGES_API': True,\n",
    "        \n",
    "        # Output format options\n",
    "        'OUTPUT_FORMAT': 'jsonl',  # 'csv', 'json', 'jsonl', 'sqlite'\n",
    "        'OUTPUT_COMPRESSION': 'gzip',  # None, 'gzip', 'bz2'\n",
    "        'ENABLE_DATA_DEDUPLICATION': True,  # Remove duplicate data\n",
    "        'MAX_FILE_SIZE_MB': 100,  # Split files when they exceed this size\n",
    "        \n",
    "        # Record update behavior\n",
    "        'UPDATE_EXISTING_RECORDS': True,  # Merge new data with existing records\n",
    "        'FORCE_REPROCESS': False,  # Ignore checkpoint and process all domains\n",
    "        \n",
    "        # Shutdown check interval (seconds)\n",
    "        'SHUTDOWN_CHECK_INTERVAL': 1.0\n",
    "    }\n",
    "    \n",
    "    # Performance presets\n",
    "    if preset == \"speed\":\n",
    "        base_config.update({\n",
    "            'MAX_CONCURRENCY': 50,\n",
    "            'REQUEST_TIMEOUT_SECONDS': 8,\n",
    "            'MAX_PAGES_PER_DOMAIN': 50,\n",
    "            'ENABLE_WHOIS': False,  # Disable slow WHOIS lookups\n",
    "            'ENABLE_PAGES_API': False,  # Only check posts, not pages\n",
    "            'batch_save_interval': 10,\n",
    "            'MAX_EMAILS_IN_OUTPUT': 5,\n",
    "            'MAX_SOCIALS_IN_OUTPUT': 8,\n",
    "            'COMMON_PAGES': [\"\", \"contact\", \"about\"],  # Fewer pages to crawl\n",
    "            'OUTPUT_COMPRESSION': 'gzip',\n",
    "            'MAX_FILE_SIZE_MB': 50,  # Smaller files for speed\n",
    "            'ENABLE_DATA_DEDUPLICATION': True\n",
    "        })\n",
    "    elif preset == \"complete\":\n",
    "        base_config.update({\n",
    "            'MAX_CONCURRENCY': 10,\n",
    "            'REQUEST_TIMEOUT_SECONDS': 20,\n",
    "            'MAX_PAGES_PER_DOMAIN': 500,\n",
    "            'batch_save_interval': 3,\n",
    "            'MAX_EMAILS_IN_OUTPUT': 20,\n",
    "            'MAX_SOCIALS_IN_OUTPUT': 25,\n",
    "            'COMMON_PAGES': [\"\", \"contact\", \"contact-us\", \"support\", \"help\", \"about\", \"about-us\", \"team\", \"services\", \"blog\", \"news\"]\n",
    "        })\n",
    "    elif preset == \"minimal\":\n",
    "        # Optimized for smallest file sizes\n",
    "        base_config.update({\n",
    "            'MAX_CONCURRENCY': 30,\n",
    "            'REQUEST_TIMEOUT_SECONDS': 10,\n",
    "            'MAX_PAGES_PER_DOMAIN': 20,\n",
    "            'ENABLE_WHOIS': False,\n",
    "            'ENABLE_CRAWLING': False,  # Skip crawling to reduce data\n",
    "            'ENABLE_PAGES_API': False,  # Only posts\n",
    "            'batch_save_interval': 20,\n",
    "            'MAX_EMAILS_IN_OUTPUT': 3,\n",
    "            'MAX_SOCIALS_IN_OUTPUT': 5,\n",
    "            'COMMON_PAGES': [\"\", \"contact\"],\n",
    "            'OUTPUT_COMPRESSION': 'gzip',\n",
    "            'MAX_FILE_SIZE_MB': 25,\n",
    "            'ENABLE_DATA_DEDUPLICATION': True,\n",
    "            'OUTPUT_FORMAT': 'jsonl'  # More compact than CSV\n",
    "        })\n",
    "    # \"balanced\" preset uses base_config as-is\n",
    "    \n",
    "    return base_config\n",
    "\n",
    "# ---------- Graceful Shutdown Handler ----------\n",
    "class GracefulShutdown:\n",
    "    def __init__(self, shutdown_file: str):\n",
    "        self.shutdown_requested = False\n",
    "        self.shutdown_file = shutdown_file\n",
    "        self._setup_signal_handlers()\n",
    "    \n",
    "    def _setup_signal_handlers(self):\n",
    "        \"\"\"Setup signal handlers for graceful shutdown\"\"\"\n",
    "        try:\n",
    "            signal.signal(signal.SIGINT, self._signal_handler)\n",
    "            signal.signal(signal.SIGTERM, self._signal_handler)\n",
    "        except Exception:\n",
    "            pass  # Signal handling may not work in all environments (e.g., Jupyter)\n",
    "    \n",
    "    def _signal_handler(self, signum, frame):\n",
    "        \"\"\"Handle shutdown signals\"\"\"\n",
    "        print(f\"\\nðŸ›‘ Received shutdown signal ({signum}). Finishing current batch...\")\n",
    "        self.shutdown_requested = True\n",
    "    \n",
    "    def should_shutdown(self) -> bool:\n",
    "        \"\"\"Check if shutdown has been requested via signal or file\"\"\"\n",
    "        if self.shutdown_requested:\n",
    "            return True\n",
    "        \n",
    "        # Check for shutdown file\n",
    "        if os.path.exists(self.shutdown_file):\n",
    "            print(f\"ðŸ›‘ Shutdown file '{self.shutdown_file}' detected. Finishing current batch...\")\n",
    "            self.shutdown_requested = True\n",
    "            try:\n",
    "                os.remove(self.shutdown_file)\n",
    "            except Exception:\n",
    "                pass\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "# ---------- Data Validation ----------\n",
    "def validate_and_load_data(input_file):\n",
    "    \"\"\"Validate input file and load domain data\"\"\"\n",
    "    if not os.path.exists(input_file):\n",
    "        #print(f\"âŒ Error: Input file '{input_file}' not found!\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "        #print(f\"âœ… Loaded {len(df)} domains from {input_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error reading CSV file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"âŒ Error: CSV file is empty!\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Normalize domain list\n",
    "    domains = []\n",
    "    raw_domains = []\n",
    "    try:\n",
    "        if \"domain\" in [c.lower() for c in df.columns]:\n",
    "            col = [c for c in df.columns if c.lower() == \"domain\"][0]\n",
    "            raw_domains = [str(x).strip() for x in df[col].dropna().tolist() if str(x).strip()]\n",
    "        else:\n",
    "            raw_domains = [str(x).strip() for x in df.iloc[:,0].dropna().tolist() if str(x).strip()]\n",
    "        \n",
    "        # Normalize each domain and track invalid ones\n",
    "        invalid_domains = []\n",
    "        for raw_domain in raw_domains:\n",
    "            normalized = normalize_domain(raw_domain)\n",
    "            if normalized:\n",
    "                domains.append(normalized)\n",
    "                if normalized != raw_domain.lower():\n",
    "                    print(f\"ðŸ“ Normalized '{raw_domain}' â†’ '{normalized}'\")\n",
    "            else:\n",
    "                invalid_domains.append(raw_domain)\n",
    "        \n",
    "        # Report invalid domains\n",
    "        if invalid_domains:\n",
    "            print(f\"âš ï¸ Skipped {len(invalid_domains)} invalid domain(s): {', '.join(invalid_domains[:5])}\")\n",
    "            if len(invalid_domains) > 5:\n",
    "                print(f\"   ... and {len(invalid_domains) - 5} more\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error parsing domains: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not domains:\n",
    "        print(\"âŒ No domains found in the CSV file!\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    return domains\n",
    "\n",
    "# ---------- Helper Functions ----------\n",
    "def normalize_domain(domain_input: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize domain input to extract clean domain name.\n",
    "    Handles URLs like https://domain.com, https://www.domain.com, etc.\n",
    "    \n",
    "    Args:\n",
    "        domain_input (str): Raw domain input (can be URL or plain domain)\n",
    "        \n",
    "    Returns:\n",
    "        str: Clean domain name (e.g., \"example.com\")\n",
    "    \"\"\"\n",
    "    if not domain_input or not isinstance(domain_input, str):\n",
    "        return \"\"\n",
    "    \n",
    "    domain_input = domain_input.strip()\n",
    "    \n",
    "    # If it looks like a URL, parse it\n",
    "    if domain_input.startswith(('http://', 'https://', 'www.')):\n",
    "        try:\n",
    "            # Add protocol if missing but starts with www\n",
    "            if domain_input.startswith('www.'):\n",
    "                domain_input = 'https://' + domain_input\n",
    "            \n",
    "            parsed = urlparse(domain_input)\n",
    "            domain = parsed.netloc.lower()\n",
    "            \n",
    "            # Remove www. prefix if present\n",
    "            if domain.startswith('www.'):\n",
    "                domain = domain[4:]\n",
    "            \n",
    "            # Remove port numbers from parsed netloc\n",
    "            if ':' in domain and domain.count(':') == 1:\n",
    "                domain = domain.split(':')[0]\n",
    "                \n",
    "            return domain\n",
    "        except Exception:\n",
    "            # If URL parsing fails, try to extract domain manually\n",
    "            pass\n",
    "    \n",
    "    # Clean up common prefixes/suffixes\n",
    "    domain = domain_input.lower()\n",
    "    \n",
    "    # Remove protocol prefixes\n",
    "    for prefix in ['https://', 'http://', 'www.']:\n",
    "        if domain.startswith(prefix):\n",
    "            domain = domain[len(prefix):]\n",
    "    \n",
    "    # Remove trailing slashes and paths\n",
    "    if '/' in domain:\n",
    "        domain = domain.split('/')[0]\n",
    "    \n",
    "    # Remove port numbers (but avoid breaking IPv6 addresses)\n",
    "    if ':' in domain and domain.count(':') == 1:  # Only single colon (not IPv6)\n",
    "        domain = domain.split(':')[0]\n",
    "    \n",
    "    # Basic validation - should contain at least one dot\n",
    "    if '.' not in domain or len(domain) < 3:\n",
    "        return \"\"\n",
    "    \n",
    "    return domain\n",
    "\n",
    "def normalize_url(base_url: str, href: str) -> str:\n",
    "    url = urljoin(base_url, href)\n",
    "    parsed = urlparse(url)\n",
    "    clean = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    if clean.endswith(\"/\") and len(clean) > len(f\"{parsed.scheme}://{parsed.netloc}/\"):\n",
    "        clean = clean[:-1]\n",
    "    return clean\n",
    "\n",
    "def is_internal(target_url: str, root_domain: str) -> bool:\n",
    "    try:\n",
    "        netloc = urlparse(target_url).netloc.lower()\n",
    "        return netloc.endswith(root_domain.lower())\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def looks_like_binary(path: str) -> bool:\n",
    "    return any(path.lower().endswith(ext) for ext in (\n",
    "        \".jpg\",\".jpeg\",\".png\",\".gif\",\".webp\",\".svg\",\".ico\",\".pdf\",\".zip\",\".rar\",\n",
    "        \".7z\",\".mp4\",\".mp3\",\".avi\",\".mov\",\".wmv\",\".mkv\",\".doc\",\".docx\",\".xls\",\n",
    "        \".xlsx\",\".ppt\",\".pptx\",\".css\",\".js\",\".woff\",\".woff2\",\".ttf\",\".eot\",\".otf\"\n",
    "    ))\n",
    "\n",
    "def get_date_patterns_and_cutoff(recent_days_threshold):\n",
    "    \"\"\"Get date patterns and recent cutoff date\"\"\"\n",
    "    RECENT_CUTOFF = datetime.now() - timedelta(days=recent_days_threshold)\n",
    "    \n",
    "    DATE_PATTERNS = [\n",
    "        r\"(20[0-9]{2})[-/\\.](0[1-9]|1[0-2])(?:[-/\\.](0[1-9]|[12][0-9]|3[01]))?\",\n",
    "        r\"(?:(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec|January|February|March|April|June|July|August|September|October|November|December)\\s+)([0-9]{1,2},?\\s+)?(20[0-9]{2})\",\n",
    "    ]\n",
    "    \n",
    "    MONTH_MAP = {\n",
    "        'jan':1,'january':1, 'feb':2,'february':2,'mar':3,'march':3,'apr':4,'april':4,\n",
    "        'may':5,'jun':6,'june':6,'jul':7,'july':7,'aug':8,'august':8,'sep':9,'sept':9,'september':9,\n",
    "        'oct':10,'october':10,'nov':11,'november':11,'dec':12,'december':12\n",
    "    }\n",
    "    \n",
    "    return DATE_PATTERNS, MONTH_MAP, RECENT_CUTOFF\n",
    "\n",
    "def any_recent_date_in_text(text: str, recent_days_threshold: int) -> bool:\n",
    "    DATE_PATTERNS, MONTH_MAP, RECENT_CUTOFF = get_date_patterns_and_cutoff(recent_days_threshold)\n",
    "    \n",
    "    try:\n",
    "        for m in re.finditer(DATE_PATTERNS[0], text):\n",
    "            y = int(m.group(1)); mo = int(m.group(2)); d = m.group(3)\n",
    "            day = int(d) if d else 1\n",
    "            dt = datetime(y, mo, day)\n",
    "            if dt >= RECENT_CUTOFF and dt <= datetime.now() + timedelta(days=3):\n",
    "                return True\n",
    "        for m in re.finditer(DATE_PATTERNS[1], text, flags=re.IGNORECASE):\n",
    "            month_name = m.group(1).lower(); mo = MONTH_MAP.get(month_name, None)\n",
    "            y = int(m.group(3)); day = 1\n",
    "            if m.group(2):\n",
    "                digits = re.findall(r\"[0-9]{1,2}\", m.group(2))\n",
    "                if digits: day = int(digits[0])\n",
    "            if mo:\n",
    "                dt = datetime(y, mo, min(day,28))\n",
    "                if dt >= RECENT_CUTOFF and dt <= datetime.now() + timedelta(days=3):\n",
    "                    return True\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def check_workspace(domain: str) -> str:\n",
    "    try:\n",
    "        answers = dns.resolver.resolve(domain, \"MX\")\n",
    "        for rdata in answers:\n",
    "            if \"google.com\" in str(rdata.exchange).lower():\n",
    "                return \"Yes\"\n",
    "        return \"No\"\n",
    "    except:\n",
    "        return \"NA\"\n",
    "\n",
    "def get_whois(domain: str):\n",
    "    try:\n",
    "        w = whois.whois(domain)\n",
    "        country = w.get(\"country\") or \"NA\"\n",
    "        created = w.get(\"creation_date\", \"NA\")\n",
    "        updated = w.get(\"updated_date\", \"NA\")\n",
    "        if isinstance(created, list) and created: created = created[0]\n",
    "        if isinstance(updated, list) and updated: updated = updated[0]\n",
    "        return (country or \"NA\", str(created) if created else \"NA\", str(updated) if updated else \"NA\")\n",
    "    except:\n",
    "        return (\"NA\",\"NA\",\"NA\")\n",
    "\n",
    "# ---------- WordPress API Functions ----------\n",
    "def get_last_wp_entry(domain: str, content_type: str = \"posts\"):\n",
    "    \"\"\"\n",
    "    Fetch last created and last modified WordPress post/page via REST API.\n",
    "\n",
    "    Args:\n",
    "        domain (str): Domain name (example.com).\n",
    "        content_type (str): \"posts\" or \"pages\".\n",
    "\n",
    "    Returns:\n",
    "        dict with created and modified info, or None values if unavailable.\n",
    "    \"\"\"\n",
    "    base_urls = [\n",
    "        f\"https://{domain}/wp-json/wp/v2/{content_type}\",\n",
    "        f\"https://www.{domain}/wp-json/wp/v2/{content_type}\"\n",
    "    ]\n",
    "\n",
    "    result = {\n",
    "        \"domain\": domain,\n",
    "        \"type\": content_type,\n",
    "        \"last_created\": None,\n",
    "        \"last_modified\": None,\n",
    "        \"status\": \"API not available\"\n",
    "    }\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Referer\": \"https://google.com\",\n",
    "    }\n",
    "    for base_url in base_urls:\n",
    "        try:\n",
    "            # Get latest created (orderby=date)\n",
    "            created_url = f\"{base_url}?per_page=1&orderby=date&order=desc\"\n",
    "            #print(f\"Fetching latest created entry for {domain} via {created_url}\")\n",
    "            r_created = requests.get(created_url, timeout=10, headers=headers)\n",
    "            \n",
    "            # Get latest modified (orderby=modified)\n",
    "            modified_url = f\"{base_url}?per_page=1&orderby=modified&order=desc\"\n",
    "            #print(f\"Fetching latest modified entry for {domain} via {modified_url}\")\n",
    "            r_modified = requests.get(modified_url, timeout=10, headers=headers)\n",
    "\n",
    "            #print(r_created.status_code, r_modified.status_code)\n",
    "            if r_created.status_code == 200 and r_modified.status_code == 200:\n",
    "                data_created = r_created.json()\n",
    "                data_modified = r_modified.json()\n",
    "\n",
    "                if isinstance(data_created, list) and len(data_created) > 0:\n",
    "                    item = data_created[0]\n",
    "                    result[\"last_created\"] = {\n",
    "                        \"title\": item.get(\"title\", {}).get(\"rendered\"),\n",
    "                        \"slug\": item.get(\"slug\"),\n",
    "                        \"date\": item.get(\"date\"),         # created date\n",
    "                        \"modified\": item.get(\"modified\"), # last updated\n",
    "                        \"link\": item.get(\"link\"),\n",
    "                        \"api_url\": created_url\n",
    "                    }\n",
    "\n",
    "                if isinstance(data_modified, list) and len(data_modified) > 0:\n",
    "                    item = data_modified[0]\n",
    "                    result[\"last_modified\"] = {\n",
    "                        \"title\": item.get(\"title\", {}).get(\"rendered\"),\n",
    "                        \"slug\": item.get(\"slug\"),\n",
    "                        \"date\": item.get(\"date\"),         # created date\n",
    "                        \"modified\": item.get(\"modified\"), # last updated\n",
    "                        \"link\": item.get(\"link\"),\n",
    "                        \"api_url\": modified_url\n",
    "                    }\n",
    "\n",
    "                result[\"status\"] = \"success\"\n",
    "                return result  # success, stop trying further URLs\n",
    "\n",
    "        except Exception:\n",
    "            continue  # try next base URL\n",
    "\n",
    "    return result\n",
    "\n",
    "# ---------- Async Site Crawler ----------\n",
    "class SiteScrapeResult:\n",
    "    def __init__(self):\n",
    "        self.emails = set()\n",
    "        self.socials = set()\n",
    "        self.has_contact_form = False\n",
    "        self.pages_crawled = 0\n",
    "        self.any_recent_blog_hint = False\n",
    "        self.contact_page_link = None\n",
    "        self.about_page_link = None\n",
    "\n",
    "async def fetch_page(session, url: str, timeout: int) -> str:\n",
    "    try:\n",
    "        async with session.get(url, timeout=timeout, headers={\"User-Agent\": get_config()['USER_AGENT']}) as resp:\n",
    "            if resp.status != 200: return \"\"\n",
    "            text = await resp.text(errors=\"ignore\")\n",
    "            return text[:2_000_000]\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "async def crawl_domain(session, domain: str, config: dict) -> SiteScrapeResult:\n",
    "    result = SiteScrapeResult()\n",
    "\n",
    "    # Candidate base URLs\n",
    "    base_variants = [\n",
    "        f\"https://{domain}\",\n",
    "        f\"http://{domain}\",\n",
    "        f\"https://www.{domain}\",\n",
    "        f\"http://www.{domain}\",\n",
    "    ]\n",
    "\n",
    "    async def resolve_base(base_url: str) -> Optional[str]:\n",
    "        \"\"\"Fetch base URL once to see if it works & resolve redirects.\"\"\"\n",
    "        try:\n",
    "            resp = await session.get(base_url, allow_redirects=True, timeout=10)\n",
    "            if resp.status < 400:\n",
    "                return str(resp.url)  # final resolved URL after redirects\n",
    "        except Exception:\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "    # Step 1: Find the first working base URL\n",
    "    resolved_base = None\n",
    "    for base in base_variants:\n",
    "        resolved = await resolve_base(base)\n",
    "        if resolved:\n",
    "            resolved_base = resolved.rstrip(\"/\")\n",
    "            break\n",
    "\n",
    "    if not resolved_base:\n",
    "        return result  # nothing worked\n",
    "\n",
    "    # Step 2: Build candidate URLs using resolved base\n",
    "    candidates = [f\"{resolved_base}/{p}\".rstrip(\"/\") for p in config['COMMON_PAGES']]\n",
    "\n",
    "    visited = set()\n",
    "    sem = asyncio.Semaphore(5)\n",
    "\n",
    "    async def process(url):\n",
    "        if url in visited:\n",
    "            return\n",
    "        visited.add(url)\n",
    "\n",
    "        async with sem:\n",
    "            html = await fetch_page(session, url, config['REQUEST_TIMEOUT_SECONDS'])\n",
    "\n",
    "        if not html:\n",
    "            return\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # --- Extract emails ---\n",
    "        for m in re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", html):\n",
    "            if not m.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".gif\", \".webp\", \".svg\")):\n",
    "                result.emails.add(m)\n",
    "\n",
    "        # --- Extract social links ---\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a[\"href\"]\n",
    "            if any(s in href for s in config['SOCIAL_DOMAINS']):\n",
    "                result.socials.add(href)\n",
    "\n",
    "        # --- Detect contact forms ---\n",
    "        if not result.has_contact_form:\n",
    "            for f in soup.find_all(\"form\"):\n",
    "                blob = \" \".join([\n",
    "                    (f.get(\"id\") or \"\"),\n",
    "                    (f.get(\"name\") or \"\"),\n",
    "                    f.get_text(\" \", strip=True).lower()\n",
    "                ])\n",
    "                if any(k in blob for k in (\"contact\", \"message\", \"email\")):\n",
    "                    result.has_contact_form = True\n",
    "                    break\n",
    "\n",
    "        # --- Detect contact and about pages ---\n",
    "        if not result.contact_page_link:\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                href = a[\"href\"]\n",
    "                if \"contact\" in href.lower():\n",
    "                    result.contact_page_link = href\n",
    "                    break\n",
    "        if not result.about_page_link:\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                href = a[\"href\"]\n",
    "                if \"about\" in href.lower():\n",
    "                    result.about_page_link = href\n",
    "                    break\n",
    "\n",
    "    # Step 3: Process candidate pages concurrently\n",
    "    await asyncio.gather(*(process(url) for url in candidates))\n",
    "\n",
    "    return result\n",
    "\n",
    "# ---------- Main Processing Function ----------\n",
    "async def process_domains(domains, config):\n",
    "    \"\"\"Process all domains and return results\"\"\"\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    checkpoint_file = config['checkpoint_file']\n",
    "    batch_save_interval = config['batch_save_interval']\n",
    "    checkpoint_data = {}\n",
    "    \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "    \n",
    "    processed_domains = checkpoint_data.get('processed_domains', [])\n",
    "    failed_domains = checkpoint_data.get('failed_domains', [])\n",
    "    next_domain_index = checkpoint_data.get('next_domain_index', 0)\n",
    "    \n",
    "    shutdown_handler = GracefulShutdown(config['shutdown_file'])\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for i, domain in enumerate(domains[next_domain_index:], next_domain_index):\n",
    "            # Skip domains only if not forcing reprocess\n",
    "            if not config.get('FORCE_REPROCESS', False):\n",
    "                if domain in processed_domains or domain in failed_domains:\n",
    "                    continue\n",
    "            \n",
    "            # Check for shutdown at the beginning of each iteration\n",
    "            if shutdown_handler.should_shutdown():\n",
    "                print(f\"ðŸ›‘ Shutdown requested. Stopping at domain {i+1}/{len(domains)}\")\n",
    "                break\n",
    "            \n",
    "            domain_start_time = time.time()\n",
    "            #print(f\"Processing {i+1}/{len(domains)}: {domain}\")\n",
    "            \n",
    "            try:\n",
    "                # WHOIS and DNS timing\n",
    "                whois_start = time.time()\n",
    "                workspace = check_workspace(domain) if config['ENABLE_DNS_MX'] else \"NA\"\n",
    "                country, created, updated = get_whois(domain) if config['ENABLE_WHOIS'] else (\"NA\",\"NA\",\"NA\")\n",
    "                whois_dns_time = time.time() - whois_start\n",
    "               \n",
    "                # Crawling timing\n",
    "                crawl_start = time.time()\n",
    "                crawl_result = await crawl_domain(session, domain, config) if config['ENABLE_CRAWLING'] else SiteScrapeResult()\n",
    "                crawl_time = time.time() - crawl_start\n",
    "\n",
    "                emails_out = \"; \".join(sorted(crawl_result.emails)[:config['MAX_EMAILS_IN_OUTPUT']]) if crawl_result.emails else \"NA\"\n",
    "                socials_out = \"; \".join(sorted(crawl_result.socials)[:config['MAX_SOCIALS_IN_OUTPUT']]) if crawl_result.socials else \"NA\"\n",
    "\n",
    "                # WordPress API timing\n",
    "                wp_start = time.time()\n",
    "                \n",
    "                posts_data = get_last_wp_entry(domain, \"posts\") if (config['ENABLE_WORDPRESS_API'] and config['ENABLE_POSTS_API']) else {\"status\": \"disabled\", \"last_created\": None, \"last_modified\": None}\n",
    "                pages_data = get_last_wp_entry(domain, \"pages\") if (config['ENABLE_WORDPRESS_API'] and config['ENABLE_PAGES_API']) else {\"status\": \"disabled\", \"last_created\": None, \"last_modified\": None}\n",
    "                wp_time = time.time() - wp_start\n",
    "\n",
    "                # Prepare posts columns\n",
    "                posts_api_status = posts_data[\"status\"]\n",
    "                posts_last_created_link = posts_data[\"last_created\"][\"link\"] if posts_data[\"last_created\"] else \"NA\"\n",
    "                posts_last_created_title = posts_data[\"last_created\"][\"title\"] if posts_data[\"last_created\"] else \"NA\"\n",
    "                posts_last_created_date = posts_data[\"last_created\"][\"date\"] if posts_data[\"last_created\"] else \"NA\"\n",
    "                posts_last_modified_title = posts_data[\"last_modified\"][\"title\"] if posts_data[\"last_modified\"] else \"NA\"\n",
    "                posts_last_modified_date = posts_data[\"last_modified\"][\"modified\"] if posts_data[\"last_modified\"] else \"NA\"\n",
    "\n",
    "                # Prepare pages columns\n",
    "                pages_api_status = pages_data[\"status\"]\n",
    "                pages_last_created_link = pages_data[\"last_created\"][\"link\"] if pages_data[\"last_created\"] else \"NA\"\n",
    "                pages_last_created_title = pages_data[\"last_created\"][\"title\"] if pages_data[\"last_created\"] else \"NA\"\n",
    "                pages_last_created_date = pages_data[\"last_created\"][\"date\"] if pages_data[\"last_created\"] else \"NA\"\n",
    "                pages_last_modified_title = pages_data[\"last_modified\"][\"title\"] if pages_data[\"last_modified\"] else \"NA\"\n",
    "                pages_last_modified_date = pages_data[\"last_modified\"][\"modified\"] if pages_data[\"last_modified\"] else \"NA\"\n",
    "\n",
    "                results.append({\n",
    "                    \"Domain\": domain,\n",
    "                    \"Google Workspace\": workspace,\n",
    "                    \"Country of Origin\": country,\n",
    "                    \"Domain Created\": created,\n",
    "                    \"Domain Last Modified\": updated,\n",
    "                    \"Pages Crawled\": crawl_result.pages_crawled,\n",
    "                    \"Has Contact Form\": \"Yes\" if crawl_result.has_contact_form else \"No\",\n",
    "                    \"Emails\": emails_out,\n",
    "                    \"Social Links\": socials_out,\n",
    "                    \"Contact Page Link\": crawl_result.contact_page_link or \"NA\",\n",
    "                    \"About Page Link\": crawl_result.about_page_link or \"NA\",\n",
    "                    \"Posts API Status\": posts_api_status,\n",
    "                    \"Posts Last Created Title\": posts_last_created_title,\n",
    "                    \"Posts Last Created Link\": posts_last_created_link,\n",
    "                    \"Posts Last Created Date\": posts_last_created_date,\n",
    "                    \"Posts Last Modified Title\": posts_last_modified_title,\n",
    "                    \"Posts Last Modified Date\": posts_last_modified_date,\n",
    "                    \"Pages API Status\": pages_api_status,\n",
    "                    \"Pages Last Created Title\": pages_last_created_title,\n",
    "                    \"Pages Last Created Link\": pages_last_created_link,\n",
    "                    \"Pages Last Created Date\": pages_last_created_date,\n",
    "                    \"Pages Last Modified Title\": pages_last_modified_title,\n",
    "                    \"Pages Last Modified Date\": pages_last_modified_date\n",
    "                })\n",
    "                \n",
    "                # Mark as successfully processed\n",
    "                processed_domains.append(domain)\n",
    "                total_time = time.time() - domain_start_time\n",
    "                \n",
    "                # Show which operations were performed\n",
    "                ops_performed = []\n",
    "                if config['ENABLE_DNS_MX']: ops_performed.append(\"DNS\")\n",
    "                if config['ENABLE_WHOIS']: ops_performed.append(\"WHOIS\")\n",
    "                if config['ENABLE_CRAWLING']: ops_performed.append(\"Crawl\")\n",
    "                if config['ENABLE_WORDPRESS_API'] and (config['ENABLE_POSTS_API'] or config['ENABLE_PAGES_API']): \n",
    "                    ops_performed.append(\"WP-API\")\n",
    "                \n",
    "                ops_str = f\" [{', '.join(ops_performed)}]\" if ops_performed else \"\"\n",
    "                #print(f\"âœ… Successfully processed {domain} in {total_time:.2f}s{ops_str} (WHOIS/DNS: {whois_dns_time:.2f}s, Crawl: {crawl_time:.2f}s, WP: {wp_time:.2f}s)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                total_time = time.time() - domain_start_time\n",
    "                print(f\"âŒ Error processing {domain} in {total_time:.2f}s: {e}\")\n",
    "                # Mark as failed but still track progress\n",
    "                failed_domains.append(domain)\n",
    "                print(f\"âš ï¸ Marked {domain} as failed, will not retry\")\n",
    "            \n",
    "            # Update checkpoint data regardless of success/failure\n",
    "            checkpoint_data['processed_domains'] = processed_domains\n",
    "            checkpoint_data['failed_domains'] = failed_domains\n",
    "            checkpoint_data['next_domain_index'] = i + 1\n",
    "            checkpoint_data['last_updated'] = datetime.now().isoformat()\n",
    "            \n",
    "            # Save checkpoint and results at regular intervals\n",
    "            if (i + 1) % batch_save_interval == 0:\n",
    "                with open(checkpoint_file, 'w') as f:\n",
    "                    json.dump(checkpoint_data, f, indent=2)\n",
    "                if results:  # Only save if there are successful results\n",
    "                    save_results(results, config)\n",
    "                    results = []\n",
    "                print(f\"ðŸ’¾ Checkpoint saved at domain {i+1} (Success: {len(processed_domains)}, Failed: {len(failed_domains)})\")\n",
    "    \n",
    "    # Save any remaining results\n",
    "    if results:\n",
    "        save_results(results, config)\n",
    "    \n",
    "    # Final checkpoint save\n",
    "    with open(checkpoint_file, 'w') as f:\n",
    "        json.dump(checkpoint_data, f, indent=2)\n",
    "    \n",
    "    # Summary\n",
    "    total_processing_time = time.time() - start_time\n",
    "    total_processed = len(processed_domains) + len(failed_domains)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Processing Summary:\")\n",
    "    print(f\"   âœ… Successful: {len(processed_domains)}\")\n",
    "    print(f\"   âŒ Failed: {len(failed_domains)}\")\n",
    "    print(f\"   ðŸ“ˆ Total processed: {total_processed}/{len(domains)}\")\n",
    "    print(f\"   â° Total time: {total_processing_time:.2f} seconds ({total_processing_time/60:.1f} minutes)\")\n",
    "    \n",
    "    if total_processed > 0:\n",
    "        avg_time_per_domain = total_processing_time / total_processed\n",
    "        #print(f\"   ðŸ“Š Average time per domain: {avg_time_per_domain:.2f} seconds\")\n",
    "        \n",
    "        # Estimate remaining time if not complete\n",
    "        remaining_domains = len(domains) - total_processed\n",
    "        if remaining_domains > 0:\n",
    "            estimated_remaining_time = remaining_domains * avg_time_per_domain\n",
    "            #print(f\"   ðŸ”® Estimated time for remaining {remaining_domains} domains: {estimated_remaining_time/60:.1f} minutes\")\n",
    "            #print(f\"   ðŸŽ¯ Estimated completion: {(datetime.now() + timedelta(seconds=estimated_remaining_time)).strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    # Performance rate\n",
    "    if total_processing_time > 0:\n",
    "        domains_per_second = total_processed / total_processing_time\n",
    "        domains_per_minute = domains_per_second * 60\n",
    "        #print(f\"   ðŸš€ Processing rate: {domains_per_minute:.1f} domains/minute ({domains_per_second:.2f} domains/second)\")\n",
    "    \n",
    "    # Show configuration summary\n",
    "    enabled_features = []\n",
    "    if config['ENABLE_DNS_MX']: enabled_features.append(\"DNS MX\")\n",
    "    if config['ENABLE_WHOIS']: enabled_features.append(\"WHOIS\")\n",
    "    if config['ENABLE_CRAWLING']: enabled_features.append(\"Crawling\")\n",
    "    if config['ENABLE_WORDPRESS_API'] and config['ENABLE_POSTS_API']: enabled_features.append(\"Posts API\")\n",
    "    if config['ENABLE_WORDPRESS_API'] and config['ENABLE_PAGES_API']: enabled_features.append(\"Pages API\")\n",
    "    \n",
    "    #print(f\"   ðŸ”§ Features enabled: {', '.join(enabled_features)}\")\n",
    "    \n",
    "    # Clean up checkpoint if all domains processed\n",
    "    #if total_processed >= len(domains):\n",
    "        #try:\n",
    "            #os.remove(checkpoint_file)\n",
    "            #print(\"ðŸ§¹ Checkpoint file cleaned up - processing complete!\")\n",
    "        #except:\n",
    "            #pass\n",
    "\n",
    "# ---------- File Management Functions ----------\n",
    "def rotate_output_file(original_file):\n",
    "    \"\"\"Create a new output file when the current one gets too large\"\"\"\n",
    "    import time\n",
    "    timestamp = int(time.time())\n",
    "    base_name, ext = os.path.splitext(original_file)\n",
    "    new_file = f\"{base_name}_{timestamp}{ext}\"\n",
    "    return new_file\n",
    "\n",
    "def compress_file(file_path, compression='gzip'):\n",
    "    \"\"\"Compress a file using the specified compression method\"\"\"\n",
    "    if compression == 'gzip':\n",
    "        import gzip\n",
    "        with open(file_path, 'rb') as f_in:\n",
    "            with gzip.open(f\"{file_path}.gz\", 'wb') as f_out:\n",
    "                f_out.writelines(f_in)\n",
    "        os.remove(file_path)\n",
    "        return f\"{file_path}.gz\"\n",
    "    elif compression == 'bz2':\n",
    "        import bz2\n",
    "        with open(file_path, 'rb') as f_in:\n",
    "            with bz2.open(f\"{file_path}.bz2\", 'wb') as f_out:\n",
    "                f_out.writelines(f_in)\n",
    "        os.remove(file_path)\n",
    "        return f\"{file_path}.bz2\"\n",
    "    return file_path\n",
    "\n",
    "def optimize_data_for_storage(results, config):\n",
    "    \"\"\"Optimize data before storage to reduce file size\"\"\"\n",
    "    optimized_results = []\n",
    "    \n",
    "    for result in results:\n",
    "        optimized = {}\n",
    "        for key, value in result.items():\n",
    "            # Convert long strings to abbreviated versions\n",
    "            if isinstance(value, str):\n",
    "                # Truncate very long titles/content\n",
    "                if 'Title' in key and len(value) > 100:\n",
    "                    value = value[:97] + \"...\"\n",
    "                # Remove redundant \"NA\" values for optional fields\n",
    "                elif value == \"NA\" and key in ['Contact Page Link', 'About Page Link', 'Emails (unique)', 'Social Links (unique)']:\n",
    "                    continue  # Skip storing NA values for optional fields\n",
    "                # Compress URLs\n",
    "                elif 'Link' in key and value.startswith('http'):\n",
    "                    # Keep only essential part of URL\n",
    "                    from urllib.parse import urlparse\n",
    "                    parsed = urlparse(value)\n",
    "                    value = f\"{parsed.netloc}{parsed.path}\"\n",
    "            \n",
    "            optimized[key] = value\n",
    "        \n",
    "        # Only include domains with meaningful data\n",
    "        if config.get('ENABLE_DATA_DEDUPLICATION', True):\n",
    "            # Skip if this is just a basic domain with no additional info\n",
    "            meaningful_fields = [k for k, v in optimized.items() \n",
    "                               if k != 'Domain' and v not in ['NA', 'No', 'disabled', '']]\n",
    "            if len(meaningful_fields) >= 2:  # At least 2 meaningful fields\n",
    "                optimized_results.append(optimized)\n",
    "        else:\n",
    "            optimized_results.append(optimized)\n",
    "    \n",
    "    return optimized_results\n",
    "\n",
    "# ---------- Save Results Function ----------\n",
    "def save_results(results, config):\n",
    "    \"\"\"Save results in the specified output format with update capability\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    # Check file size and rotate if necessary\n",
    "    output_file = config['output_file']\n",
    "    max_size_mb = config.get('MAX_FILE_SIZE_MB', 100)\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "        if file_size_mb > max_size_mb:\n",
    "            output_file = rotate_output_file(output_file)\n",
    "            config['output_file'] = output_file\n",
    "            #print(f\"ðŸ“ File size exceeded {max_size_mb}MB, rotating to: {output_file}\")\n",
    "        \n",
    "    output_file = config['output_file']\n",
    "    output_format = config.get('OUTPUT_FORMAT', 'csv').lower()\n",
    "    update_existing = config.get('UPDATE_EXISTING_RECORDS', True)\n",
    "    \n",
    "    try:\n",
    "        # Optimize data before processing\n",
    "        optimized_results = optimize_data_for_storage(results, config)\n",
    "        new_df = pd.DataFrame(optimized_results)\n",
    "        expected_cols = [\n",
    "            \"Domain\",\n",
    "            \"Google Workspace\",\"Country of Origin\",\"Domain Created\",\"Domain Last Modified\",\n",
    "            \"Pages Crawled (Count)\",\"Has Contact Form\",\"Emails (unique)\",\"Social Links (unique)\",\n",
    "            \"Contact Page Link\",\"About Page Link\",\n",
    "            \"Posts API Status\",\"Posts Last Created Title\",\"Posts Last Created Link\",\"Posts Last Created Date\",\"Posts Last Modified Title\",\"Posts Last Modified Date\",\n",
    "            \"Pages API Status\",\"Pages Last Created Title\",\"Pages Last Created Link\",\"Pages Last Created Date\",\"Pages Last Modified Title\",\"Pages Last Modified Date\"\n",
    "        ]\n",
    "        for col in expected_cols:\n",
    "            if col not in new_df.columns: new_df[col] = \"NA\"\n",
    "        new_df = new_df[expected_cols]\n",
    "        \n",
    "        if output_format == 'csv':\n",
    "            file_exists = os.path.exists(output_file)\n",
    "            \n",
    "            if update_existing and file_exists:\n",
    "                # Read existing data and merge\n",
    "                try:\n",
    "                    existing_df = pd.read_csv(output_file)\n",
    "                    # Merge new data with existing, updating records by Domain\n",
    "                    merged_df = merge_domain_records(existing_df, new_df)\n",
    "                    merged_df.to_csv(output_file, index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "                    #print(f\"âœ… Results updated in {output_file} ({output_format.upper()} format) - {len(new_df)} records processed\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Error reading existing file for update: {e}. Appending instead.\")\n",
    "                    new_df.to_csv(output_file, index=False, mode='a', header=False, quoting=csv.QUOTE_MINIMAL)\n",
    "            else:\n",
    "                # Append mode for incremental saving\n",
    "                new_df.to_csv(output_file, index=False, mode='a', header=not file_exists, quoting=csv.QUOTE_MINIMAL)\n",
    "                #print(f\"âœ… Results saved to {output_file} ({output_format.upper()} format)\")\n",
    "            \n",
    "        elif output_format == 'json':\n",
    "            # For JSON, read existing data and merge\n",
    "            existing_data = []\n",
    "            if os.path.exists(output_file):\n",
    "                try:\n",
    "                    with open(output_file, 'r') as f:\n",
    "                        existing_data = json.load(f)\n",
    "                except:\n",
    "                    existing_data = []\n",
    "            \n",
    "            if update_existing and existing_data:\n",
    "                # Convert to DataFrame for merging\n",
    "                existing_df = pd.DataFrame(existing_data)\n",
    "                merged_df = merge_domain_records(existing_df, new_df)\n",
    "                final_data = merged_df.to_dict('records')\n",
    "            else:\n",
    "                final_data = existing_data + new_df.to_dict('records')\n",
    "            \n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(final_data, f, indent=2, default=str)\n",
    "            #print(f\"âœ… Results saved to {output_file} ({output_format.upper()} format)\")\n",
    "                \n",
    "        elif output_format == 'jsonl':\n",
    "            # For JSONL, we need to read all lines, merge, and rewrite\n",
    "            if update_existing and os.path.exists(output_file):\n",
    "                existing_records = []\n",
    "                try:\n",
    "                    with open(output_file, 'r') as f:\n",
    "                        for line in f:\n",
    "                            if line.strip():\n",
    "                                existing_records.append(json.loads(line))\n",
    "                    \n",
    "                    existing_df = pd.DataFrame(existing_records)\n",
    "                    merged_df = merge_domain_records(existing_df, new_df)\n",
    "                    \n",
    "                    # Rewrite the entire file\n",
    "                    with open(output_file, 'w') as f:\n",
    "                        for record in merged_df.to_dict('records'):\n",
    "                            f.write(json.dumps(record, default=str) + '\\n')\n",
    "                    #print(f\"âœ… Results saved to {output_file} ({output_format.upper()} format)\")\n",
    "                    \n",
    "                    # Apply compression if enabled\n",
    "                    compression = config.get('OUTPUT_COMPRESSION')\n",
    "                    if compression and compression != 'None':\n",
    "                        compressed_file = compress_file(output_file, compression)\n",
    "                        print(f\"ðŸ—œï¸ File compressed: {compressed_file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Error updating JSONL file: {e}. Appending instead.\")\n",
    "                    with open(output_file, 'a') as f:\n",
    "                        for record in new_df.to_dict('records'):\n",
    "                            f.write(json.dumps(record, default=str) + '\\n')\n",
    "            else:\n",
    "                # Append mode\n",
    "                with open(output_file, 'a') as f:\n",
    "                    for record in new_df.to_dict('records'):\n",
    "                        f.write(json.dumps(record, default=str) + '\\n')\n",
    "                #print(f\"âœ… Results saved to {output_file} ({output_format.upper()} format)\")\n",
    "                    \n",
    "        elif output_format == 'sqlite':\n",
    "            import sqlite3\n",
    "            db_file = output_file.replace('.csv', '.db') if output_file.endswith('.csv') else output_file + '.db'\n",
    "            conn = sqlite3.connect(db_file)\n",
    "            \n",
    "            if update_existing:\n",
    "                # Use REPLACE to update existing records\n",
    "                new_df.to_sql('domains', conn, if_exists='replace', index=False, method='multi')\n",
    "            else:\n",
    "                new_df.to_sql('domains', conn, if_exists='append', index=False)\n",
    "            \n",
    "            conn.close()\n",
    "            output_file = db_file  # Update for logging\n",
    "            print(f\"âœ… Results saved to {output_file} ({output_format.upper()} format)\")\n",
    "            \n",
    "        else:\n",
    "            # Fallback to CSV\n",
    "            file_exists = os.path.exists(output_file)\n",
    "            new_df.to_csv(output_file, index=False, mode='a', header=not file_exists, quoting=csv.QUOTE_MINIMAL)\n",
    "            print(f\"âœ… Results saved to {output_file} (CSV format)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving results: {e}\")\n",
    "        # Fallback to CSV append mode\n",
    "        try:\n",
    "            file_exists = os.path.exists(config['output_file'])\n",
    "            new_df.to_csv(config['output_file'], index=False, mode='a', header=not file_exists, quoting=csv.QUOTE_MINIMAL)\n",
    "            #print(f\"âœ… Fallback: Results saved to {config['output_file']} (CSV format)\")\n",
    "        except Exception as e2:\n",
    "            print(f\"âŒ Critical error: Could not save results: {e2}\")\n",
    "\n",
    "def merge_domain_records(existing_df, new_df):\n",
    "    \"\"\"Merge new domain records with existing ones, updating fields that have new data\"\"\"\n",
    "    if existing_df.empty:\n",
    "        return new_df\n",
    "    \n",
    "    if new_df.empty:\n",
    "        return existing_df\n",
    "    \n",
    "    # Ensure both DataFrames have the same columns\n",
    "    all_cols = list(set(existing_df.columns) | set(new_df.columns))\n",
    "    for col in all_cols:\n",
    "        if col not in existing_df.columns:\n",
    "            existing_df[col] = \"NA\"\n",
    "        if col not in new_df.columns:\n",
    "            new_df[col] = \"NA\"\n",
    "    \n",
    "    # Reorder columns to match\n",
    "    existing_df = existing_df[all_cols]\n",
    "    new_df = new_df[all_cols]\n",
    "    \n",
    "    # Create a merged DataFrame\n",
    "    merged_df = existing_df.copy()\n",
    "    \n",
    "    for _, new_row in new_df.iterrows():\n",
    "        domain = new_row['Domain']\n",
    "        existing_mask = merged_df['Domain'] == domain\n",
    "        \n",
    "        if existing_mask.any():\n",
    "            # Update existing record - only update fields that are not \"NA\" or \"disabled\" in new data\n",
    "            existing_idx = merged_df[existing_mask].index[0]\n",
    "            for col in new_df.columns:\n",
    "                if col != 'Domain':  # Don't update the domain name itself\n",
    "                    new_value = new_row[col]\n",
    "                    # Update if new value is not NA/disabled and is different from existing\n",
    "                    if (new_value not in [\"NA\", \"disabled\", \"\"] and \n",
    "                        str(new_value).strip() != \"\" and \n",
    "                        new_value != merged_df.loc[existing_idx, col]):\n",
    "                        merged_df.loc[existing_idx, col] = new_value\n",
    "        else:\n",
    "            # Add new record\n",
    "            merged_df = pd.concat([merged_df, new_row.to_frame().T], ignore_index=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# ---------- Main Execution Function ----------\n",
    "async def main(preset: str = \"balanced\", custom_config: dict = None):\n",
    "    \"\"\"Main execution function with preset support\"\"\"\n",
    "    config = get_config(preset)\n",
    "    \n",
    "    # Apply custom configuration overrides if provided\n",
    "    if custom_config:\n",
    "        config.update(custom_config)\n",
    "    \n",
    "    print(f\"ðŸš€ Starting WordPress Domain Analysis with '{preset}' preset\")\n",
    "    print(f\"ðŸ“‹ Configuration:\")\n",
    "    print(f\"   â€¢ Max Concurrency: {config['MAX_CONCURRENCY']}\")\n",
    "    print(f\"   â€¢ Request Timeout: {config['REQUEST_TIMEOUT_SECONDS']}s\")\n",
    "    print(f\"   â€¢ Batch Save Interval: {config['batch_save_interval']}\")\n",
    "    print(f\"   â€¢ Output Format: {config.get('OUTPUT_FORMAT', 'csv').upper()}\")\n",
    "    print(f\"   â€¢ Update Existing Records: {'Yes' if config.get('UPDATE_EXISTING_RECORDS', True) else 'No'}\")\n",
    "    print(f\"   â€¢ Force Reprocess: {'Yes' if config.get('FORCE_REPROCESS', False) else 'No'}\")\n",
    "    \n",
    "    # Show enabled features\n",
    "    enabled_features = []\n",
    "    if config['ENABLE_DNS_MX']: enabled_features.append(\"DNS MX\")\n",
    "    if config['ENABLE_WHOIS']: enabled_features.append(\"WHOIS\")\n",
    "    if config['ENABLE_CRAWLING']: enabled_features.append(\"Crawling\")\n",
    "    if config['ENABLE_WORDPRESS_API'] and config['ENABLE_POSTS_API']: enabled_features.append(\"Posts API\")\n",
    "    if config['ENABLE_WORDPRESS_API'] and config['ENABLE_PAGES_API']: enabled_features.append(\"Pages API\")\n",
    "    print(f\"   â€¢ Features: {', '.join(enabled_features)}\")\n",
    "    \n",
    "    print(f\"   â€¢ Shutdown file: {config['shutdown_file']} (create this file to stop gracefully)\")\n",
    "    print()\n",
    "    \n",
    "    domains = validate_and_load_data(config['input_file'])\n",
    "    await process_domains(domains, config)\n",
    "\n",
    "# ---------- Entry Point ----------\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main(\"balanced\", {\"ENABLE_PAGES_API\": True}))\n",
    "\n",
    "    # Run 1: Basic info only\n",
    "#await main(\"balanced\", {\"ENABLE_WORDPRESS_API\": False})\n",
    "\n",
    "# Run 2: Add WordPress Posts\n",
    "#await main(\"balanced\", {\"ENABLE_WHOIS\": False, \"ENABLE_CRAWLING\": False, \"ENABLE_PAGES_API\": False})\n",
    "\n",
    "# Run 3: Add WordPress Pages  \n",
    "#await main(\"balanced\", {\"ENABLE_WHOIS\": False, \"ENABLE_CRAWLING\": False, \"ENABLE_POSTS_API\": False})"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
